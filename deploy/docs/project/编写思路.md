

api形式多样，包括session、device等，也可以infer Single/Multi Pic/Video/Dataset简单，sync/async
core文件夹包含多模型（串/并）、多前后端、同步/异步、cache等策略
model文件夹包含前处理、推理、后处理，这部分具体硬件具体实现，按照标准API格式封装实现对接core

后续各个平台的bin/include/lib文件单独抽取出来放入thirdparty中作为编译依赖，src/model文件夹中的各个硬件平台创建各自creator来注册，达到所有的平台同一份代码

各个平台在对应的分支名字修改和提交
共同的部分，也就是除了src/model文件夹以外部分在common分支修改和提交，如果是别的分支修改记得cherry-pick回common，同时同步到全部平台分支

再往上可添加core每个组件对应的python对象和接口，或者Java接口

编写每个后端的cmake文件，查找对应的头文件和库文件，这样编写对应cmakefile的时候只需要find packget

### 分支说明：
master
    dev
        common->明确管理的目录，是公用部分代码分支，所有的修改都需要sync到各分支
            core
            benchmark
            docker
        onnx
        bmnn    ->硬件厂商名字
            se5     ->硬件平台
                custom1     ->客户1客制化维护分支
                custom2
            sc5
        ascend
        cambricon
            cambricon_370
            cambricon_270
            cambricon_220
        openvino
        nvidia
        libtorch
        alg ->算法分支
            classify
            detect
                ssd
                retina
                yolo
            pose
                yolox-pose
            3ddetect
            seg

    release
        tag v0.1
        tag v0.2

dev下的各个厂商下分出去具体客户可单独维护，记录好是从哪个tag开始分出去的，后续如果公共部分修改有需要update到次分支再单独cherry-pick过来

代码架构上硬件平台相关必须单独文件夹存放，均采用注册制（注册对应的基类、creator），编译可选，git管理中硬件平台只可以上传对应文件夹内容，别的都贵common分支上传。(这样的好处是以后同步common分支代码不会出现代码冲突)

tag命名规范：主版本号-次版本号-硬件厂商-硬件系列-硬件版本号

### 参考：
1）easyDK的core部分，主要实现多device/thread/engine，共model实现
2）mmdeploy的注册部分，主要实现pre/infer/post注册，新算法注册
3）deepstream的meta部分，使得级联模型的信息可以流通共享
4）openvino最新版的autoDevice部分，device资源分配均衡

### 目录说明：
.
|-- CMakeLists.txt
|-- README.md
|-- benchmark   //benchmark代码
|-- cmake       //存放对应平台的cmake编译选项
|-- data        //测试数据，包括模型、图片、视频
|-- demo        //简单推理代码，包括cpp/python，未来可迁移到test
|-- docker      //构建docker的dockerfile和对应的安装脚本
|-- docs
|-- include
|-- python      //将C++封装为python接口
|-- release     //release模板代码和要求
|-- sample      //示例代码
|-- scripts     //常用脚本
|-- src
|   |-- api     //对外api
|   |-- app     //一些应用接口，比如pic/video/dataset推理的接口
|   |-- codec   //编解码相关
|   |-- common  
|   |-- core    //多模型多engine多batch相关逻辑
|   |-- register    //注册模块，包含算法注册和设备注册，
|   |   |-- device   //具体硬件
|   |   |   |-- codec   //具体硬件解码接口实现
|   |   |   |-- processor   //具体硬件实现的算法模块，包含前处理、推理、后处理
|   |   |-- alg   //具体算法,，包含classify、detect、pose、3D detect、seg
|   `-- tracker
|-- test        //测试代码，和src相对应
|-- thirdparty  //第三方代码
|-- tools       //工具，包括模型转换和量化工具

实现方式：


### 新SDK怎么解决之前的问题：
- 推理过程中，临时的pic需要推理怎么解决：
    设置帧为最高登记，dispatch的时候判断为最高登记优先推理，然后sync

- 多模型串联：
    前面模型的结果存入PackagePtr，下一个模型推理判断继续判断，解析operate_on_class_id解析需要crop的对象坐标

- 多模型并联：
    硬件资源分配

- 如何分发硬件资源：
    如果是开始就知道模型数量和视频流路数：平均分、权重分（根据模型的计算大小）
    后续添加视频流或后续添加模型：
    预留可能随时回来的推理图片临时需求：
        把计算单元抽象为计算能力，通过预先压测得到对应设备在对应模型运行时间得到其计算能力

- 模型推理怎么判断自己处于下游推理，需要用到之前的结果数据


- 怎么方便以后插入新模型：
    新模型需要做的是前后处理，前处理通过以后的resize/crop/color等组合，后处理通过手写cpu方式
    到具体的硬件，前处理是抽象对象不会区分硬件，后处理可能会有差异，某些硬件会有加速算子实现，则单独实现和注册


- 怎么解决内存，显存：
    前后处理从预留的内存池中申请

- 内存池资源怎么分配，分配多大
    是否每个模型创建都要分配
    
- 如果是nvidia这种，单模型已经执行效率很高，并行反而降低效率，如果加锁推理串行



- 如果是小模型，怎么确定并行推理效率高还是串行
        

- 如何保证代码质量：
test文件夹包含多种类型测试，包含单元测试、联调测试、release测试、性能测试


- 新硬件平台注册需要做的工作:
cmake添加对应的编译选项和依赖的库和头文件等信息
在core/register/device编写对应的文件
在test编写对应的单元测试文件，复用联调和release文件
编译到对应的硬件平台进行运行和测试，性能测试
具体参考《新硬件平台注册.md》文档


- 新算法注册需要做的工作：
在core/register/alg编写对应的文件，包含前处理、后处理，pipeline
编译，在各个平台进行运行测试
算法类的任务的根据模型类别和参数创建对于的processor，构建pipeline


- 怎么解决代码因为多平台导致越来越臃肿，难以维护
多平台差异文件才有register方式，均在放在register文件夹，开发对应平台需要切换对应的git分支，开发完毕后合并到dev分支
代码编译采用cmake选项编译，没有打开的选项对应的源文件不会参与编译，不会因此导致编译后的文件十分庞大和带有大量无关代码。

## 性能方面优化
- latency和through优先
同步和异步接口的问题

- 如何保持和原框架一样的context和stream


- 如何保持原有框架的异步接口


- 如何auto context和stream
根据硬件资源确定多少worker，也可以设置，不同的worker分配到不同的stream
前处理、推理使用不同stream

- 如何auto device
多少个device创建多少个infer_server

- 如何调试调度情况
使用工具nsight

- engine策略
1）分不同阶段不同队列管理
2）队列消费者跟资源有关，多少个设备就多少个worker
3）凑batch


## 如何和后处理模块衔接

## 如何调用多精度单元IP
比如比特的int8和fp32是不同的单元，理论上可以分开跑

## 如何加入后处理模块
作为一种processor加入engine